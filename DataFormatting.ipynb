{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json as JSON\n",
    "#from ipynb.fs.full.InsertsDelComparisons import  map_, INDICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>ks</th>\n",
       "      <th>recipe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-04 03:28:18.613319</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662261900176, 'character': 'Shift'}...</td>\n",
       "      <td>Brown 1 pound of hamburger meat. Drain the gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-04 03:29:37.124556</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662261900176, 'character': 'Shift'}...</td>\n",
       "      <td>1) Brown 1 pound of hamburger meat. Drain the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-04 03:29:47.816111</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662261900176, 'character': 'Shift'}...</td>\n",
       "      <td>1) Brown 1 pound of hamburger meat. Drain the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-04 03:33:03.555075</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662262224600, 'character': '1'}, {'...</td>\n",
       "      <td>1) Cook chicken as desired (boiled, pan seared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-04 03:33:30.062465</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662262224600, 'character': '1'}, {'...</td>\n",
       "      <td>1) Cook chicken as desired (boiled, pan seared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-09-04 03:34:08.666681</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662262224600, 'character': '1'}, {'...</td>\n",
       "      <td>1) Cook chicken as desired (boiled, pan seared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-09-04 03:35:33.869167</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662262452515, 'character': '2'}, {'...</td>\n",
       "      <td>28 oz or so of potatoes cubed\\r\\n1 8oz of crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-09-04 03:36:08.454643</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662262452515, 'character': '2'}, {'...</td>\n",
       "      <td>28 oz or so of potatoes cubed\\r\\n1 8oz of crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-09-04 03:36:43.142187</td>\n",
       "      <td>55ae64defdf99b3f864653e7</td>\n",
       "      <td>[{'time': 1662262452515, 'character': '2'}, {'...</td>\n",
       "      <td>28 oz or so of potatoes cubed\\r\\n1 8oz of crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-09-04 14:01:31.746981</td>\n",
       "      <td>55d22025cc2b18000c0b9d9c</td>\n",
       "      <td>[{'time': 1662298866744, 'character': 'I'}, {'...</td>\n",
       "      <td>To serve 4 people (your family!)\\r\\n\\r\\n\\r\\n-Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   event_date                   user_id  \\\n",
       "0  2022-09-04 03:28:18.613319  55ae64defdf99b3f864653e7   \n",
       "1  2022-09-04 03:29:37.124556  55ae64defdf99b3f864653e7   \n",
       "2  2022-09-04 03:29:47.816111  55ae64defdf99b3f864653e7   \n",
       "3  2022-09-04 03:33:03.555075  55ae64defdf99b3f864653e7   \n",
       "4  2022-09-04 03:33:30.062465  55ae64defdf99b3f864653e7   \n",
       "5  2022-09-04 03:34:08.666681  55ae64defdf99b3f864653e7   \n",
       "6  2022-09-04 03:35:33.869167  55ae64defdf99b3f864653e7   \n",
       "7  2022-09-04 03:36:08.454643  55ae64defdf99b3f864653e7   \n",
       "8  2022-09-04 03:36:43.142187  55ae64defdf99b3f864653e7   \n",
       "9  2022-09-04 14:01:31.746981  55d22025cc2b18000c0b9d9c   \n",
       "\n",
       "                                                  ks  \\\n",
       "0  [{'time': 1662261900176, 'character': 'Shift'}...   \n",
       "1  [{'time': 1662261900176, 'character': 'Shift'}...   \n",
       "2  [{'time': 1662261900176, 'character': 'Shift'}...   \n",
       "3  [{'time': 1662262224600, 'character': '1'}, {'...   \n",
       "4  [{'time': 1662262224600, 'character': '1'}, {'...   \n",
       "5  [{'time': 1662262224600, 'character': '1'}, {'...   \n",
       "6  [{'time': 1662262452515, 'character': '2'}, {'...   \n",
       "7  [{'time': 1662262452515, 'character': '2'}, {'...   \n",
       "8  [{'time': 1662262452515, 'character': '2'}, {'...   \n",
       "9  [{'time': 1662298866744, 'character': 'I'}, {'...   \n",
       "\n",
       "                                              recipe  \n",
       "0  Brown 1 pound of hamburger meat. Drain the gre...  \n",
       "1  1) Brown 1 pound of hamburger meat. Drain the ...  \n",
       "2  1) Brown 1 pound of hamburger meat. Drain the ...  \n",
       "3  1) Cook chicken as desired (boiled, pan seared...  \n",
       "4  1) Cook chicken as desired (boiled, pan seared...  \n",
       "5  1) Cook chicken as desired (boiled, pan seared...  \n",
       "6  28 oz or so of potatoes cubed\\r\\n1 8oz of crea...  \n",
       "7  28 oz or so of potatoes cubed\\r\\n1 8oz of crea...  \n",
       "8  28 oz or so of potatoes cubed\\r\\n1 8oz of crea...  \n",
       "9  To serve 4 people (your family!)\\r\\n\\r\\n\\r\\n-Y...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/keystrokes-recipes.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "\n",
    "The platform the users used for writing recipes is a ML based review system on recipes users write. The platform is called __RELEX__ and was designed to study the behavior of users. In fact, the users were tasked with writing three recipes each and they were all divided into 5 groups. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Group 1 | Group 2 | \n",
    "|:---: | :---: | \n",
    " Without Adaptive Feedback | Without Adaptive Feedback | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note some users wrote less than 3 and others wrote more than 3 (at least from what i've seen in the data). We will filter out samples considered as the 4th or 5th recipe.\n",
    "\n",
    "\n",
    "We have 73  users (each represented by a ```user_id```) and 450 sets of keystrokes.\n",
    "Each set of keystrokes is a users revision on what they wrote previously or the start of a new recipe.  A scenario is when using the platform:\n",
    "- User x writes a first version of a recipe, clicks finished button (registers as a set of keystrokes in our dataset)\n",
    "- According to his group, the platform may or may not present some suggestions\n",
    "- The user modifies their text and submits again (registers as a second set of keystrokes in our dataset)\n",
    "- If the user is done, starts the second recipe, else revises again and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we define keywords as characters appearing in the dataset that correspond to a keyboard action. We also define ```noisy_punct``` as noisy ponctuation characters that we want to remove from certain analysis we will make. We also copy the original data to a new file we will later modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS = ['Alt', 'ArrowDown', 'ArrowLeft', 'ArrowRight', 'ArrowUp', 'Backspace', 'CapsLock', 'Control', 'Delete', 'End', 'Enter', 'Home', 'Meta', 'PageUp', 'PageDown', 'PrintScreen','Shift', 'Tab']\n",
    "noisy_punct = [',', '.', '-', ':', '(', ')']\n",
    "#create a copy of the dataset to another csv file\n",
    "csv_filename = 'data/keystrokes-recipes-modified.csv'\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```keystrokes-recipes.csv``` is the original data and we keep it in case we want to look back at one moment\n",
    "- ```keystrokes-recipes-modified.csv``` is the modified data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and sorting\n",
    "\n",
    "Our data consists of a csv file with event dates, user ids, keystrokes and the recipes they wrote.\n",
    "We clean all the data by working throught the keystrokes first.\n",
    "\n",
    "* We group the characters into the word written and separate between important keywords typed such as backspace, shift, enter etc. The sequence ['shift', 'p', 'e', 'r'] becomes ['shift', 'per'] \n",
    "* We sort the data by user id then event date to get a better idea of every recipe every student has written and the time they took."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data\n",
    "\n",
    "The first step is to collect all the keystrokes in the dataset. Pandas considers `df['ks']` as a string so we use the `ast` library to convert the string to json.\n",
    "\n",
    "The next step is to group words together and separate them from keywords and we work between each whitespace.\n",
    " \n",
    "So for example this entry: \n",
    "```{'time': 1662252404346, 'character': 'Shift'}, {'time': 1662252404376, 'character': 'f'}, {'time': 1662252404505, 'character': 'i'}, {'time': 16622524046700, 'character': ' '}``` \n",
    "\n",
    "gives the following output: \n",
    "```{'time': 1662252404346, 'word': 'Shift'}, {'time': 1662252404505, 'word': 'fi'}```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keystrokes = df['ks'].values.tolist()\n",
    "keystrokes = list(map(lambda j: ast.literal_eval(j), keystrokes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a white space at the end of each set of keystrokes to facilitate data formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = []\n",
    "for i, s in enumerate(keystrokes):\n",
    "   s = list(filter(lambda _ : _ is not None,s))\n",
    "   last_entry = s[-1]\n",
    "   s.append({\"time\" : last_entry['time'], \"character\": \" \"})\n",
    "   ks.append(s)\n",
    "ks = pd.DataFrame(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seq(chars):\n",
    "    return \"\".join(list(filter(lambda _ : _ not in KEYWORDS, chars)))\n",
    "\n",
    "def separate_entry(json_values):\n",
    "    new_data = []\n",
    "    last_whitespace = 0\n",
    "    characters = [arr[1] for arr in json_values]\n",
    "    for i, (time, character) in enumerate(json_values):\n",
    "        if character.isspace():\n",
    "            word = characters[last_whitespace: i]\n",
    "            if not any(i in word for i in KEYWORDS):\n",
    "                new_data.append({'time': time, 'word': \"\".join(word)})\n",
    "            else: \n",
    "                new_data.append({'time': time, 'word': find_seq(word)})\n",
    "            last_whitespace = i+1\n",
    "        elif character in KEYWORDS:\n",
    "            new_data.append({'time': time, 'word': character})\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "arr = []\n",
    "for jsonf in ks.values:\n",
    "    sub_arr = []\n",
    "    for d in jsonf:\n",
    "        if d is not None:\n",
    "            sub_arr.append([d[\"time\"], d[\"character\"]])\n",
    "    arr.append(sub_arr)\n",
    "\n",
    "result = []\n",
    "for jsonf in arr:\n",
    "    result.append(separate_entry(jsonf))\n",
    "with open(\"data/new_data.json\", \"w\") as f:\n",
    "    JSON.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we will format the data for the ```separate_entry``` function and when everything is computed, it dumps all the data in a new json file: ```new_data.json``` in the ```data``` directory.\n",
    "\n",
    "```separate_entry``` computes the words between each space character, all the while separating words from keywords. It uses the function ```find_seq``` to separate the characters from keywords so it allows to isolate words between each whitespaces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the CSV file\n",
    "\n",
    "We just modify the keystroke data for each row of the original data in ```keystrokes-recipes.csv``` but apply it to ```keystrokes-recipes-modified.csv```. \n",
    "$\\textbf{In the original dataset, we have 5 groups. But we are focusing on 2 so we only add the data from the two groups}$\n",
    "We use this secondary dataset which was provided, which maps users to their groups. We focus on users from groups 2 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = pd.read_json('data/new_data.json').values.tolist()\n",
    "users_to_groups = dict(pd.read_csv('data/groupmatching.csv').filter(['user_id', 'group']).values)\n",
    "\n",
    "for i, json in enumerate(jsons):\n",
    "    jsons[i]= list(filter(lambda _ : _ is not None, json))\n",
    "\n",
    "dframe = df.copy()\n",
    "\n",
    "to_drop = []\n",
    "for i in range(len(jsons)):\n",
    "    row = dframe.iloc[i]\n",
    "    try:\n",
    "        if users_to_groups[row['user_id']] == 2 or users_to_groups[row['user_id']] == 4:\n",
    "            dframe.iloc[i]['ks'] = jsons[i]\n",
    "        else: \n",
    "           to_drop.append(i)\n",
    "    except KeyError:\n",
    "        to_drop.append(i)\n",
    "        continue\n",
    "\n",
    "dframe.drop(to_drop , inplace=True)\n",
    "dframe.to_csv(csv_filename, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting by `user_id` and `event_date`\n",
    "\n",
    "We first sort by user id in order to differentiate behaviour between different people more easily and then by event date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort \n",
    "pd.read_csv(csv_filename).sort_values(by=['user_id', 'event_date'], ascending=True).to_csv(csv_filename, index=False)\n",
    "#update the dataframe with which we work with\n",
    "df = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ast.literal_eval(df['ks'].values[i]) for i in range(len(df))]\n",
    "with open(\"data/new_data.json\", \"w\") as f:\n",
    "        JSON.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating writing sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use distance metrics between each recipe written and keep the indices to separate the different recipes. This alleviates most of the work from the previous idea -- and is more safe to use -- safer than writing my own algorithm. The previous idea consisted of checking the number of inserts between each revision and check for a spike. We were also going to check the time it took between each submission and consider a recipe as new if the difference was important.\n",
    "\n",
    "We download a glove model (similar to word2vec) which is already trained on wikipedia, where each word is represented as a 50-dimensional vector.\n",
    "\n",
    "We will separate every recipe written in sessions so that we can look what happens at each revision session for every recipe written.\n",
    "\n",
    "The algorithm works recursively. For each recipe it computes the distance with the following recipes until it finds a recipe with which $1 - distance <.995$. When it does find one, it restarts the whole process from the index of said recipe with the accumulator containing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(s):\n",
    "    res = \"\"\n",
    "    for i, char in enumerate(list(s)):\n",
    "        if char not in noisy_punct:\n",
    "            res += char    \n",
    "    res = [i.lower() for i in res.split()]\n",
    "    res = list(filter(lambda _ : _ not in noisy_punct, res))\n",
    "    return res\n",
    "\n",
    "def get_vector(s):\n",
    "    \"\"\"\n",
    "    Get the vector representation of a sentence from the model\n",
    "\n",
    "    Args:\n",
    "        s (str): text\n",
    "\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    for i in preprocess(s):\n",
    "        key = None\n",
    "        try: \n",
    "            key = model[i]\n",
    "            arr.append(key)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    arr = np.array(arr)\n",
    "    return np.sum(arr, axis=0)\n",
    "\n",
    "recipes = df['recipe'].values\n",
    "\n",
    "\n",
    "\n",
    "def compute_recipe_indices(start_index, acc):\n",
    "    \"\"\"\n",
    "    Computes the list of indices where each recipe in the dataset begins\n",
    "    Basically, user 0 writes 3 recipes:\n",
    "    starts writing at t = 0, revises once at t = 1, a second time at t = 2 and \n",
    "    starts a new recipe at t = 3, then this function will return [0, 3] \n",
    "\n",
    "    Args:\n",
    "        start_index (int): index to compare with the other recipes\n",
    "        acc (list(int)): list to return\n",
    "\n",
    "    Returns:\n",
    "        list(int) : list of indices of beginning of each recipe\n",
    "    \"\"\"\n",
    "    if start_index >= len(recipes) - 1:\n",
    "        return acc\n",
    "    vec = get_vector(recipes[start_index])\n",
    "    for i in range(start_index, len(recipes)):\n",
    "        dist = 1 - spatial.distance.cosine(vec, get_vector(recipes[i]))\n",
    "        if dist < .995:\n",
    "            acc.append(i)\n",
    "            return compute_recipe_indices(i, acc)\n",
    "\n",
    "\n",
    "recipes_indices = compute_recipe_indices(0, [0])\n",
    "\n",
    "# Out of 450 samples, we only have 14 misclassified samples that are misclassified as new recipes so the algorithm pretty effectively\n",
    "to_remove = [13, 116, 134, 156, 168, 188, 249, 255, 256, 403, 88, 90, 128, 209, 376, 379, 381, 390, 391, 393, 394,395,  444]\n",
    "add = [121, 204, 254, 336, 97, 360, 362, 392]\n",
    "for i in to_remove:\n",
    "    recipes_indices.remove(i)\n",
    "\n",
    "for i in add:\n",
    "    recipes_indices.append(i) \n",
    "\n",
    "recipes_indices = sorted(recipes_indices)\n",
    "\n",
    "rec = [df['recipe'][i] for i in recipes_indices]\n",
    "users = [df['user_id'][i] for i in recipes_indices]\n",
    "dframe = pd.DataFrame([recipes_indices, rec, users]).transpose()\n",
    "dframe.columns =['recipe index in data', 'recipe', 'user id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of indices at which there is a new recipe.\n",
    "Now we have to map the recipes to the users.\n",
    "The idea is to transform the indices: ```[0,3,6,9,11,12,...]``` $\\rightarrow$ ```[(0, [0,3,6]), (1, [9,11,12]), ... ]```\n",
    "\n",
    "However since not everyone has 3 recipes, we can't simply group every 3 recipes together as that would map some recipes to users that havent written them.\n",
    "What we do instead is use pandas methods that does everything so nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map each user to the index of each recipe they wrote\n",
    "map_ = dframe.groupby('user id')[\"recipe index in data\"].apply(list)\n",
    "\n",
    "#Collect the first indices in which each user starts writing their recipes\n",
    "#if user 0 writes at indices [0,1,2,3,4] and user 1 at indices [5,6,7,8,9] then we have [0,5]\n",
    "indices_of_first_attempts_per_user = df.groupby('user_id').head(1).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing useful variables for other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df' (DataFrame)\n",
      "Stored 'KEYWORDS' (list)\n",
      "Stored 'noisy_punct' (list)\n",
      "Stored 'ks' (DataFrame)\n",
      "Stored 'map_' (Series)\n",
      "Stored 'recipes_indices' (list)\n",
      "Stored 'users_to_groups' (dict)\n",
      "Stored 'indices_of_first_attempts_per_user' (Int64Index)\n"
     ]
    }
   ],
   "source": [
    "%store df\n",
    "%store KEYWORDS\n",
    "%store noisy_punct\n",
    "%store ks\n",
    "%store map_\n",
    "%store recipes_indices\n",
    "%store users_to_groups\n",
    "%store indices_of_first_attempts_per_user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "034e82c898f96a531aea7f463adf54ed75110b5c8a706bc29f14438863882a0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
