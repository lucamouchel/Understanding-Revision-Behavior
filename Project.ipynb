{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022 16:36:42)\n",
      "Spark context Web UI available at http://10.0.13.166:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1665916615341).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json as JSON\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "from pyspark.shell import sc\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.types import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('keystrokes-recipes.csv')\n",
    "KEYWORDS = ['Shift', 'Backspace', 'Enter', 'ArrowDown',\n",
    "            'ArrowLeft', 'ArrowRight', 'ArrowUp', 'End', 'Control', 'CapsLock']\n",
    "\n",
    "#create a copy of the dataset to another csv file\n",
    "\n",
    "df.to_csv('keystrokes-recipes-modified.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```keystrokes-recipes.csv``` is the original data and we keep it in case we want to look back at one moment\n",
    "- ```keystrokes-recipes-modified.csv``` is the modified data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and sorting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of a csv file with event dates, user ids, keystrokes and the recipes they wrote.\n",
    "We clean all the data by working throught the keystrokes first.\n",
    "\n",
    "* We group the characters into the word written and separate between important keywords typed such as backspace, shift, enter etc. The sequence ['shift', 'p', 'e', 'r'] becomes ['shift', 'per'] \n",
    "* We sort by user id to get a better idea of every recipe every student has written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we did was isolate the keystrokes to a new ```json``` file saved in ```data/all_keystrokes.json```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to group words together and separate them from keywords and we work between each whitespace.\n",
    " \n",
    "So for example this entry: \n",
    "\n",
    "```{'time': 1662252404346, 'character': 'Shift'}, {'time': 1662252404376, 'character': 'f'}, {'time': 1662252404505, 'character': 'i'}``` \n",
    "\n",
    "gives the following output: \n",
    "\n",
    "```{'time': 1662252404346, 'word': 'Shift'}, {'time': 1662252404505, 'word': 'fi'}```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seq(chars):\n",
    "    return ''.join(list(filter(lambda _ : _ not in KEYWORDS, chars)))\n",
    "\n",
    "def separate_entry(json_values):\n",
    "    new_data = []\n",
    "    last_whitespace = 0\n",
    "    characters = [arr[1] for arr in json_values]\n",
    "\n",
    "    for i, (time, character) in enumerate(json_values):\n",
    "        if character.isspace():\n",
    "            word = characters[last_whitespace: i]\n",
    "            if not any(i in word for i in KEYWORDS):\n",
    "                new_data.append({'time': time, 'word': ''.join(word)})\n",
    "            else:\n",
    "                new_data.append({'time': time, 'word': find_seq(word)})\n",
    "            last_whitespace = i+1\n",
    "\n",
    "        elif character in KEYWORDS:\n",
    "            new_data.append({'time': time, 'word': character})\n",
    "        \n",
    "        elif not character:\n",
    "            continue\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def compute():\n",
    "    new_df = pd.DataFrame(pd.read_json('data/all_keystrokes.json'))\n",
    "    arr = []\n",
    "    for jsonf in new_df.values:\n",
    "        sub_arr = []\n",
    "        for d in jsonf:\n",
    "            if d is not None:\n",
    "                sub_arr.append([d['time'], d['character']])\n",
    "        arr.append(sub_arr)\n",
    "\n",
    "    result = []\n",
    "    for jsonf in arr:\n",
    "        result.append(separate_entry(jsonf))\n",
    "\n",
    "    with open('data/new_data.json', 'w') as f:\n",
    "        JSON.dump(result, f)\n",
    "    return\n",
    "\n",
    "compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, ``` compute()``` will format the data for the ```separate_entry``` function and when everything is computed, it dumps all the data in a new json file.\n",
    "\n",
    "```separate_entry``` computes the words between each space character, all the while separating words from keywords. It uses the function ```find_seq``` to separate the characters from keywords so it allows to isolate words between each whitespaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just modify the keystroke data for each row of the original data in ```keystrokes-recipes.csv``` but apply it to ```keystrokes-recipes-modified.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = pd.read_json('data/new_data.json').values.tolist()\n",
    "\n",
    "for i, json in enumerate(jsons):\n",
    "    jsons[i]= list(filter(lambda _ : _ is not None, json))\n",
    "\n",
    "def write_to_csv_file(filename, recipes_len):\n",
    "    df = pd.read_csv(filename)\n",
    "    for i in range(recipes_len):\n",
    "        df.loc[i,'ks'] = jsons[i]\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "write_to_csv_file('keystrokes-recipes-modified.csv', len(jsons))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting by user id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sort by user id in order to differentiate behaviour between different people more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'keystrokes-recipes-modified.csv'\n",
    "pd.read_csv(csv_filename).sort_values(by='user_id', ascending=True).to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying patterns between different student recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the most common words used in the recipes and removing stopwords (they're noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 15092), (',', 7062), ('-', 3485), ('1', 2802), (')', 2406), ('2', 2135), ('add', 1905), ('minutes', 1834), ('pan', 1765), (':', 1731), ('Add', 1550), ('3', 1298), ('water', 1253), ('oil', 1231), ('4', 1207), ('heat', 1180), ('5', 1077), ('garlic', 967), ('(', 961), ('salt', 958), ('sauce', 916), ('pepper', 886), ('chicken', 820), ('cook', 783), ('6', 752), ('butter', 744), ('onion', 740), ('oven', 726), ('rice', 725), ('bowl', 717), ('Ingredients', 717), ('cheese', 694), ('large', 681), ('mix', 649), ('pasta', 613), ('cup', 604), ('10', 575), ('place', 564), ('7', 537), ('medium', 537), ('mixture', 535), ('stir', 516), ('flour', 500), ('8', 496), ('cooked', 489), ('top', 480), ('sugar', 453), ('chopped', 450), ('Place', 443), ('Step', 434), ('tomatoes', 432), ('onions', 420), ('boil', 419), ('olive', 418), ('potatoes', 417), ('one', 402), ('cream', 396), ('tbsp', 395), ('eggs', 392), ('powder', 387), ('brown', 386), ('small', 385), ('cooking', 375), ('tsp', 373), ('pot', 367), ('milk', 366), ('put', 346), ('stock', 334), ('tomato', 330), ('together', 327), ('baking', 324), ('1/2', 320), ('9', 310), ('egg', 309), ('serve', 305), ('vegetables', 305), ('half', 304), ('fry', 304), ('side', 300), ('beef', 300), ('frying', 300), ('mince', 299), ('cups', 298), ('around', 296), ('dish', 292), ('taste', 288), ('pieces', 284), ('Cook', 275), ('Put', 271), ('red', 269), ('cut', 268), ('golden', 268), ('Steps', 264), ('tablespoons', 263), ('saucepan', 259), ('white', 259), ('ingredients', 258), ('tablespoon', 257), ('low', 251), ('teaspoon', 244)]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(csv_filename)['recipe'].values\n",
    "stop_words = set(stopwords.words('english'))\n",
    "recipes_copy = list(map(lambda _ : word_tokenize(_), data.copy()))\n",
    "recipes_without_stopwords = []\n",
    "for recipe in recipes_copy:\n",
    "    recipes_without_stopwords.append([w for w in recipe if not w.lower() in stop_words])    \n",
    "\n",
    "flat_list = [item for sublist in recipes_without_stopwords for item in sublist]\n",
    "c = Counter(flat_list)\n",
    "print(c.most_common(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the recipes contain an order in which to proceed - ie 1) --- 2) --- \n",
    "Also interesting to see most of the common words are not ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 1091 recipes from 187 students. What we would like to do is either apply pattern mining for each student and their recipes to better understand each users writing. Or, we pick one recipe from each student and extract the underlying patterns each student share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our data, we want to extract the longest common patterns as the shorter ones will only consist of heavily used words such as 'the', 'and' ... \n",
    "Hence, given the running time of running the ```PrefixSpan``` algorithm using ```pyspark``` to find the patterns with ```maxPatternLength=10``` is over 10 minutes for only 5 recipes, it would take approximately 1870 minutes (1091 recipes and 187 students gives roughly 5 recipes per student) to compute patterns for each student recipes. So, we ought to only pick one from each student and apply the ```PrefixSpan``` algorithm. Regardless - applying prefix span on the written recipes results in unsatisfying output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the amount of computation of the algorithm, we simply pick the first recipe each student writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = pd.read_csv(csv_filename)\n",
    "first_recipe_each_student = sorted_df.groupby('user_id').head(1)\n",
    "one_recipe_per_student = []\n",
    "for index in first_recipe_each_student.index:\n",
    "    one_recipe_per_student.append(word_tokenize((sorted_df.loc[index]['recipe'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From, there we can try to apply the PS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----+\n",
      "|sequence              |freq|\n",
      "+----------------------+----+\n",
      "|[[and, and, and, and]]|5   |\n",
      "|[[of]]                |5   |\n",
      "|[[and, and]]          |5   |\n",
      "|[[3]]                 |5   |\n",
      "|[[1]]                 |5   |\n",
      "|[[5]]                 |5   |\n",
      "|[[with]]              |5   |\n",
      "|[[the]]               |5   |\n",
      "|[[.]]                 |5   |\n",
      "|[[in]]                |5   |\n",
      "|[[4]]                 |5   |\n",
      "|[[,]]                 |5   |\n",
      "|[[a]]                 |5   |\n",
      "|[[and]]               |5   |\n",
      "|[[2]]                 |5   |\n",
      "+----------------------+----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#restricting to 5 recipes for now - the running time is too large\n",
    "recipes = one_recipe_per_student[:5]\n",
    "\n",
    "copy = recipes.copy()\n",
    "\n",
    "range_ = range(len(copy))\n",
    "for i in range_:\n",
    "    copy[i] = Row(sequence=[recipes[i]])\n",
    "\n",
    "l = [copy[i] for i in range_]\n",
    "\n",
    "df = sc.parallelize(l).toDF()\n",
    "\n",
    "prefixSpan = PrefixSpan(minSupport=0.7, maxPatternLength=8, maxLocalProjDBSize=32000000)\n",
    "\n",
    "# Find frequent sequential patterns.\n",
    "prefixSpan.findFrequentSequentialPatterns(df).sort(desc('freq')).show(15,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't really make anything of this which makes sense bc it looks for sequential patterns - not the general idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do now is try to map each typed character as an insert action, count each backspace as a delete action and from those sequences, generate some patterns under the form (insert - insert - delete - insert ...)\n",
    "\n",
    "We can also try to check if any of the commands typed are of the form ```CTRL + C``` - ```CTRL + V``` to see if any of the students might have cheated\n",
    "\n",
    "* pending approval from thiemo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "034e82c898f96a531aea7f463adf54ed75110b5c8a706bc29f14438863882a0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
